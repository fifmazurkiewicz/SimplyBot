from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from simplybot.config import Config
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self):
        # SprawdÅº czy uÅ¼ywamy OpenRouter czy OpenAI
        if Config.OPENROUTER_API_KEY:
            # UÅ¼yj OpenRouter (gÅ‚Ã³wna konfiguracja dla LLM)
            self.llm = ChatOpenAI(
                api_key=Config.OPENROUTER_API_KEY,
                base_url=Config.OPENROUTER_BASE_URL,
                model=Config.OPENROUTER_MODEL,
                temperature=Config.TEMPERATURE,
                max_tokens=Config.MAX_TOKENS
            )
        elif Config.OPENAI_API_KEY:
            # Fallback do OpenAI (jeÅ›li brak OpenRouter)
            self.llm = ChatOpenAI(
                api_key=Config.OPENAI_API_KEY,
                model=Config.OPENAI_MODEL,
                temperature=Config.TEMPERATURE,
                max_tokens=Config.MAX_TOKENS
            )
        else:
            raise ValueError("Brak klucza API - ustaw OPENROUTER_API_KEY lub OPENAI_API_KEY")
    
    async def summarize_conversation(self, conversation: List[Dict[str, str]]) -> str:
        """Podsumowuje rozmowÄ™ i przygotowuje zapytanie do RAG"""
        try:
            # Logowanie rozpoczÄ™cia podsumowywania
            logger.info(f"ðŸ§  Rozpoczynam podsumowywanie rozmowy ({len(conversation)} wiadomoÅ›ci)")
            
            # Konwertuj rozmowÄ™ na tekst
            conversation_text = "\n".join([
                f"{msg['role']}: {msg['content']}" 
                for msg in conversation
            ])
            
            # Logowanie ostatniej wiadomoÅ›ci uÅ¼ytkownika
            last_user_message = next((msg['content'] for msg in reversed(conversation) if msg['role'] == 'user'), '')
            last_message_preview = last_user_message[:10] + "..." if len(last_user_message) > 10 else last_user_message
            logger.info(f"ðŸ’¬ Ostatnia wiadomoÅ›Ä‡ uÅ¼ytkownika: '{last_message_preview}'")
            
            system_prompt = """
            JesteÅ› ekspertem w analizie rozmÃ³w. Twoim zadaniem jest:
            1. PodsumowaÄ‡ gÅ‚Ã³wne punkty rozmowy
            2. ZidentyfikowaÄ‡ kluczowe pytania lub problemy uÅ¼ytkownika
            3. PrzygotowaÄ‡ konkretne zapytanie do bazy wiedzy
            
            Odpowiedz w formacie:
            PODSUMOWANIE: [krÃ³tkie podsumowanie]
            PYTANIE: [konkretne zapytanie do bazy wiedzy]
            """
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"Rozmowa:\n{conversation_text}")
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Logowanie podsumowania
            summary_preview = response.content[:10] + "..." if len(response.content) > 10 else response.content
            logger.info(f"ðŸ“ Podsumowanie wygenerowane: '{summary_preview}'")
            
            return response.content
            
        except Exception as e:
            logger.error(f"BÅ‚Ä…d podczas podsumowywania rozmowy: {e}")
            return "Nie udaÅ‚o siÄ™ podsumowaÄ‡ rozmowy"
    
    async def answer_with_context(self, question: str, context_docs: List[Dict[str, Any]]) -> str:
        """Odpowiada na pytanie uÅ¼ywajÄ…c kontekstu z dokumentÃ³w"""
        try:
            # Logowanie rozpoczÄ™cia generowania odpowiedzi
            question_preview = question[:10] + "..." if len(question) > 10 else question
            logger.info(f"ðŸ¤– Generowanie odpowiedzi dla pytania: '{question_preview}' (kontekst: {len(context_docs)} dokumentÃ³w)")
            
            if not context_docs:
                logger.warning("âš ï¸ Brak dokumentÃ³w kontekstowych - zwracam domyÅ›lnÄ… odpowiedÅº")
                return "Nie posiadam informacji"
            
            # Przygotuj kontekst z dokumentÃ³w
            context_text = "\n\n".join([
                f"Dokument {i+1}:\n{doc.get('content', '')}"
                for i, doc in enumerate(context_docs)
            ])
            
            # Logowanie informacji o kontekÅ›cie
            context_preview = context_text[:50] + "..." if len(context_text) > 50 else context_text
            logger.info(f"ðŸ“š Kontekst przygotowany: '{context_preview}'")
            
            system_prompt = """
            JesteÅ› pomocnym asystentem. Odpowiadaj na pytania uÅ¼ytkownika na podstawie 
            dostarczonych dokumentÃ³w. JeÅ›li nie moÅ¼esz znaleÅºÄ‡ odpowiedzi w dokumentach, 
            odpowiedz dokÅ‚adnie: "Nie posiadam informacji". JeÅ¼eli nie zrozumiaÅ‚eÅ› pytania lub z rozmowy nie wynika Å¼adne pytanie, moÅ¼esz dopytaÄ‡ uÅ¼ytkownika o dalsze informacje. 
            
            Odpowiedzi MUSZÄ„ byÄ‡ w formacie:
            
            **TLDR:** [Jedna linia z szybkÄ…, zwiÄ™zÅ‚Ä… odpowiedziÄ…]
            
            **Opis:** [SzczegÃ³Å‚owy opis z dodatkowymi informacjami, kontekstem i wyjaÅ›nieniami]
            
            Odpowiedzi powinny byÄ‡:
            - DokÅ‚adne i oparte na faktach z dokumentÃ³w
            - TLDR powinien byÄ‡ bardzo zwiÄ™zÅ‚y (1-2 zdania)
            - Opis moÅ¼e byÄ‡ dÅ‚uÅ¼szy i zawieraÄ‡ szczegÃ³Å‚y
            - W jÄ™zyku polskim
            """
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"Kontekst:\n{context_text}\n\nPytanie: {question}")
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Logowanie wygenerowanej odpowiedzi
            answer_preview = response.content[:10] + "..." if len(response.content) > 10 else response.content
            logger.info(f"âœ… OdpowiedÅº wygenerowana: '{answer_preview}'")
            
            return response.content
            
        except Exception as e:
            logger.error(f"BÅ‚Ä…d podczas generowania odpowiedzi: {e}")
            return "Nie posiadam informacji" 